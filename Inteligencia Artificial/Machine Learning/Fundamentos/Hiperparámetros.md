links: [[001 - 020 Machine Learning|Machine Learning]]


# Hiperpar치metros

Los **hiperpar치metros** son configuraciones externas que **controlan la arquitectura** y el **comportamiento de los modelos** de ML durante su entrenamiento. A diferencia de los par치metros internos (como los pesos en redes neuronales), **no se aprenden de los datos** sino que se establecen previamente[^5][^6].

### 游늷 Caracter칤sticas clave de los hiperpar치metros:

1. **Control estructural**: Definen la complejidad del modelo (ej. n칰mero de capas en redes neuronales)[^2][^5]
2. **Regulaci칩n del aprendizaje**: Determinan c칩mo se ajustan los par치metros internos (ej. tasa de aprendizaje)[^4][^7]
3. **Prevenci칩n de sobreajuste**: Par치metros como el **dropout rate** ayudan a regularizar el modelo[^1][^2]

### Principales m칠todos de ajuste:

| M칠todo | Mecanismo | Ventajas | Desventajas |
| :-- | :-- | :-- | :-- |
| **B칰squeda en cuadr칤cula (Grid Search)** | Prueba todas las combinaciones predefinidas[^2][^4] | Exhaustivo | Costo computacional alto |
| **B칰squeda aleatoria (Random Search)** | Muestreo aleatorio del espacio de par치metros[^1][^4] | M치s eficiente que Grid Search | Puede omitir combinaciones 칩ptimas |
| **Optimizaci칩n bayesiana** | Modela probabil칤sticamente el espacio de b칰squeda[^5][^7] | Eficiente para espacios complejos | Requiere configuraci칩n experta |
| **Algoritmos evolutivos** | Usa principios de selecci칩n natural[^5] | Adaptable a diferentes problemas | Alto costo computacional |

### Proceso t칤pico de optimizaci칩n:

1. **Definici칩n del espacio de b칰squeda**: Seleccionar hiperpar치metros cr칤ticos y sus rangos (ej. n칰mero de neuronas: 10-30)[^2][^4]
2. **Elecci칩n de m칠tricas**: Usar [[Cross Validation]] para evaluar rendimiento[^1][^4]
3. **Implementaci칩n iterativa**:
    - Entrenamiento con diferentes combinaciones[^4][^7]
    - Evaluaci칩n mediante m칠tricas como precisi칩n o AUC[^1][^3]
4. **Selecci칩n final**: Elegir la combinaci칩n con mejor rendimiento en datos de validaci칩n[^4][^5]

Los hiperpar치metros cr칤ticos var칤an por modelo: en [[001 - 021 Redes Neuronales|redes neuronales]] incluyen **funciones de activaci칩n**(|ReLU, sigmoide)[^2][^3], mientras que en [[Decision Trees|치rboles de decisi칩n]] ser칤a la **profundidad m치xima**[^6]. Su correcto ajuste mejora la capacidad predictiva y reduce problemas como el sobreajuste, logrando un equilibrio 칩ptimo entre sesgo y varianza[^1][^5].

---
tags:
	#Concept #MachineLearning 

[^1]: https://pdfs.semanticscholar.org/7717/e95bead815f804c24fb66667ec9b5b3b431c.pdf

[^2]: https://pdfs.semanticscholar.org/f5f8/9aa81d02cd786db96b6df78dbbede14709ae.pdf

[^3]: https://pdfs.semanticscholar.org/913a/dbbc9ae266293696c355eb1670dcd1da7495.pdf

[^4]: https://pdfs.semanticscholar.org/c591/2631e59e6af06a0d61034fd448884f137475.pdf

[^5]: https://foqum.io/blog/termino/hiperparametro/

[^6]: https://metodotrading.com/hiperparametros/

[^7]: https://aws.amazon.com/es/what-is/hyperparameter-tuning/

[^8]: https://www.datasource.ai/es/data-science-articles/optimizacion-de-hiper-parametros-para-modelos-de-aprendizaje-automatico

[^9]: https://es.wikipedia.org/wiki/Optimizaci칩n_de_hiperpar치metros

[^10]: https://es.linkedin.com/advice/3/what-top-hyperparameter-tuning-techniques-ai-lr7ne?lang=es

[^11]: https://es.eitca.org/inteligencia-artificial/eitc-ai-gcml-google-nube-aprendizaje-autom치tico/primeros-pasos-en-el-aprendizaje-autom치tico/los-7-pasos-del-aprendizaje-autom치tico/쮺u치les-son-algunos-ejemplos-de-ajuste-de-hiperpar치metros%3F/

[^12]: https://www.paradigmadigital.com/dev/optimizando-hiper-parametros-una-perspectiva-teorica/

[^13]: https://pdfs.semanticscholar.org/f471/91be377a5a5765697f63dd7abd09ea4a8de0.pdf

[^14]: https://pdfs.semanticscholar.org/fc11/a7fa69eeef5005849cb581d197a534117831.pdf

[^15]: https://pdfs.semanticscholar.org/58ad/94db3aaa2fa2e05c84d0d4c1e99d3b0a8dfc.pdf

[^16]: https://pdfs.semanticscholar.org/2c67/59bbfa65cc4c2d8b660171863998cdb5a761.pdf

[^17]: https://scholar.google.com/citations?user=yV-zsPgAAAAJ\&hl=es

[^18]: https://pdfs.semanticscholar.org/9b22/6c8fdf520ceb29c453677db72be1fcf95287.pdf

[^19]: https://scholar.google.com/citations?user=v0yL02gAAAAJ\&hl=es

[^20]: https://pdfs.semanticscholar.org/40fa/321047c2c15024f24dc49e8e89f5d0f6ea17.pdf

[^21]: https://pdfs.semanticscholar.org/cc1a/23a291a1b813a438e9a77000a3f6a7e3e05e.pdf

[^22]: https://pdfs.semanticscholar.org/91ac/ff0ad951a7c50a21b1db2a0f947b32bdd566.pdf

[^23]: https://pdfs.semanticscholar.org/eda5/cb13db63d7fe5dae1928c0176335b6a2634b.pdf

[^24]: https://pdfs.semanticscholar.org/9fa3/f7d50c53d3b80dc2e52f1b32f8fe0c5d2c9c.pdf

[^25]: https://arxiv.org/html/2402.15039v1

[^26]: https://developers.google.com/machine-learning/crash-course/linear-regression/hyperparameters?hl=es-419

[^27]: https://iartificial.blog/glossary/hiperparametro/

[^28]: https://forum.huawei.com/enterprise/intl/es/hiperpar치metros-en-machine-learning/thread/840089-100263?isInitURL=true

[^29]: https://www.ibm.com/es-es/think/topics/hyperparameter-tuning

[^30]: https://www.youtube.com/watch?v=3Iu5m166rnE

[^31]: https://keepcoding.io/blog/que-son-hiperparametros-en-prompt-engineering/

[^32]: https://es.wikipedia.org/wiki/Hiperpar치metro_(aprendizaje_autom치tico)

[^33]: https://codificandobits.com/blog/parametros-hiperparametros-machine-learning/

[^34]: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml_dlaas_hpo.html?locale=es

[^35]: https://www.datacamp.com/es/tutorial/parameter-optimization-machine-learning-models

[^36]: https://www.mundoposgrado.com/optimizacion-de-hiperparametros-modelos-deep-learning/

[^37]: https://es.eitca.org/artificial-intelligence/eitc-ai-gcml-google-cloud-machine-learning/first-steps-in-machine-learning/the-7-steps-of-machine-learning/what-are-some-examples-of-hyperparameter-tuning/

[^38]: https://www.youtube.com/watch?v=EgklwkyieOY

[^39]: https://es.eitca.org/artificial-intelligence/eitc-ai-gcml-google-cloud-machine-learning/introduction/what-is-machine-learning/what-is-the-difference-between-hyperparameters-and-model-parameters/

[^40]: https://es.eitca.org/inteligencia-artificial/eitc-ai-gcml-google-nube-aprendizaje-autom치tico/introducci칩n/que-es-el-aprendizaje-automatico/쯈u칠-son-los-hiperpar치metros-de-los-algoritmos%3F/

