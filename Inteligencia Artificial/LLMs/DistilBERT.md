links: [[001 - 020 Inteligencia Artificial|Inteligencia Artificial]]


# DistilBERT

Versión comprimida de BERT obtenida por [[Distilación|knowledge distillation]]. 6 capas, 66M parámetros, 97% del rendimiento de BERT-base con 40% menos parámetros y 60% más rápido en inferencia.


---
tags:
	#LLM #Transformers #ModelCompression