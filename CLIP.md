# CLIP (Contrastive Languageâ€“Image Pre-training)  
Es un modelo desarrollado por **OpenAI (2021)** que **aprende a relacionar imÃ¡genes con texto**.  
Su objetivo es lograr que el modelo â€œentiendaâ€ lo que ve, no solo como pÃ­xeles, sino **en tÃ©rminos de lenguaje natural**.

ğŸ“˜ Nombre completo:
> â€œLearning Transferable Visual Models From Natural Language Supervisionâ€  
> _Alec Radford et al., 2021 (OpenAI)_
---
## CÃ³mo funciona
CLIP **entrena dos redes al mismo tiempo**:

|Parte|Entrada|Objetivo|
|---|---|---|
|ğŸ§  **Visual Encoder**|Imagen|Convierte la imagen en un vector (embedding)|
|ğŸ—£ï¸ **Text Encoder**|Texto|Convierte la descripciÃ³n en otro vector|
Durante el entrenamiento, CLIP **aprende a acercar en el espacio vectorial** las imÃ¡genes y los textos que se corresponden, y a **alejar** los que no.

ğŸ’¡ Ejemplo:
- Imagen: ğŸ–¼ï¸ una foto de un perro.
- Textos: â€œa photo of a dogâ€, â€œa photo of a catâ€, â€œa car in motionâ€.

CLIP aprende que la imagen y el texto â€œphoto of a dogâ€ deben tener **embeddings cercanos**, y los otros deben estar lejos. 
Este proceso se llama **contrastive learning** (aprendizaje contrastivo).

---
## QuÃ© representa CLIP
CLIP **aprende una representaciÃ³n multimodal** (imagen + texto).  
Eso significa que puede:
- â€œEntenderâ€ quÃ© hay en una imagen sin etiquetas especÃ­ficas.
- Ser usado para **zero-shot classification** (clasificar sin entrenamiento adicional).
- Conectar **conceptos lingÃ¼Ã­sticos** con **elementos visuales**.

Por ejemplo:  
Si le das una imagen de un gato y los textos â€œun perroâ€, â€œun gatoâ€, â€œuna sillaâ€,  
CLIP predice que el texto mÃ¡s cercano es â€œun gatoâ€.

---
## Por quÃ© es Ãºtil para tu TFM
Mi [[TFM]] busca **reconocimiento semÃ¡ntico de escenas**, donde los objetos y relaciones tienen **significado simbÃ³lico**.

CLIP es una **excelente capa de percepciÃ³n** para esto, porque:
1. **Ya asocia texto â†” imagen**, asÃ­ que puedes mapear fÃ¡cilmente los objetos detectados con **conceptos simbÃ³licos** del KG.
2. Puedes usar sus **embeddings** para crear **alineaciones semÃ¡nticas automÃ¡ticas** entre:
    
    - Labels neuronales (â€œcarâ€, â€œpersonâ€, â€œdogâ€)
        
    - Conceptos simbÃ³licos (â€œAutomÃ³vilâ€, â€œPersonaâ€, â€œPerroâ€)
        
3. Permite generar **descripciones textuales explicativas** de una escena (â€œA person driving a car near a traffic lightâ€).

ğŸ’¡ AsÃ­, CLIP puede ser el **puente natural** entre la parte neuronal y la simbÃ³lica.

---
## ğŸ§± Estructura tÃ©cnica (simplificada)

Imagen  â”€â”€â–º Visual Encoder  â”€â”                                           
						â”œâ”€â”€â–º Espacio conjunto (embedding)
 Texto â”€â”€â”€â–º Text Encoder   â”€â”€â”€â”˜

ğŸ”¹ Si las representaciones son similares â†’ la imagen y el texto coinciden.  
ğŸ”¹ Si son diferentes â†’ no coinciden.

---
## ğŸ§  En resumen

|Aspecto|CLIP|
|---|---|
|**AcrÃ³nimo**|Contrastive Languageâ€“Image Pre-training|
|**Desarrollado por**|OpenAI (2021)|
|**QuÃ© hace**|Alinea imÃ¡genes y texto en un mismo espacio semÃ¡ntico|
|**Entrenamiento**|400 millones de pares imagen-texto|
|**Utilidad**|ClasificaciÃ³n zero-shot, embeddings semÃ¡nticos, transferencia|
|**Por quÃ© te sirve**|Puente entre visiÃ³n profunda (DL) y conocimiento simbÃ³lico (KG)|

---
## ğŸ’¡ Ejemplo aplicado a tu TFM
Por ejemplo, CLIP detecta la relaciÃ³n:

> Imagen â†’ â€œa man driving a car near a traffic lightâ€

Se puede extraer tripletas semÃ¡nticas:
`(Persona, conduce, AutomÃ³vil)`
`(AutomÃ³vil, cerca_de, SemÃ¡foro)`

Y luego alinear eso con el **grafo simbÃ³lico**, que sabrÃ¡ si:
- Es coherente con las reglas del dominio.
- Faltan relaciones.
- Se puede explicar (â€œes una calle urbana porque hay coche + semÃ¡foro + peatÃ³nâ€).
