links: [[001 - 020 Machine Learning|Machine Learning]]


# M√°quinas de Vectores Soporte (SVM)

Las **M√°quinas de Vectores Soporte (SVM)** (Support Vector Machines) son un conjunto de algoritmos de aprendizaje supervisado. Su objetivo principal es encontrar un hiperplano √≥ptimo que separe las clases maximizando el margen (la "calle") entre ellas.

Son un caso particular de "Kernel Machines".

## üìå Conceptos Clave

### Clasificaci√≥n de Margen M√°ximo

El objetivo de un clasificador SVM es ajustarse a la "calle" m√°s ancha posible entre las clases.

- **Vectores de Soporte**: Son las instancias de entrenamiento que se sit√∫an en el borde de la "calle". El l√≠mite de decisi√≥n est√° totalmente determinado por estos vectores.
    
- **Sensibilidad a la Escala**: Las SVM son sensibles a las escalas de las caracter√≠sticas , por lo que es conveniente normalizar los atributos (p.ej., a rangos [-1, 1] o [0, 1]).
    

### Margen Duro vs. Margen Blando

- **Clasificaci√≥n de Margen Duro**: Impone estrictamente que todas las instancias deben estar fuera de la "calle" y en el lado correcto. Solo funciona si los datos son linealmente separables y es sensible a valores at√≠picos (_outliers_).
    
- **Clasificaci√≥n de Margen Blando**: Es un modelo m√°s flexible. Busca un equilibrio entre mantener la "calle" lo m√°s grande posible y limitar las "violaciones del margen" (instancias dentro o al otro lado de la calle).
    
    - Este equilibrio se controla mediante el **hiperpar√°metro de regularizaci√≥n `C`**.
        
    - Un valor de `C` bajo permite m√°s violaciones (m√°s permisivo con outliers).
        
    - Un valor de `C` alto permite menos violaciones (menos permisivo), pero puede llevar al [[Overfitting|sobreajuste]].
        

## Clasificaci√≥n No Lineal y el Truco del Kernel

Para manejar conjuntos de datos no lineales, un m√©todo consiste en a√±adir m√°s caracter√≠sticas (como caracter√≠sticas polin√≥micas) para hacer el conjunto de datos linealmente separable.

### El Truco del Kernel (Kernel Trick)

El **truco del kernel** es una t√©cnica matem√°tica que permite obtener el mismo resultado que si se hubieran a√±adido muchas caracter√≠sticas polin√≥micas (incluso de grado muy alto), sin tener que a√±adirlas realmente.

Esto evita la "explosi√≥n combinatoria" del n√∫mero de caracter√≠sticas. Un kernel es una funci√≥n capaz de calcular el producto punto de los vectores transformados bas√°ndose √∫nicamente en los vectores originales.

Tipos de Kernels Comunes 

- **Lineal**: $K(a,b)=a^{\top}b$ 
    
- **Polin√≥mico**: $K(a,b)=(\gamma a^{\top}b+r)^{d}$ 
    
- **RBF Gaussiano**: $K(a,b)=\exp(-\gamma||a-b||^{2})$ 
    
    - Este kernel suele funcionar muy bien y es una buena opci√≥n por defecto.
        
    - El par√°metro `gamma` ($\gamma$) tambi√©n act√∫a como regularizador. Valores altos de gamma pueden conducir a [[Overfitting|sobreajuste]].
        
- **Sigmoide**: $K(a,b)=\tanh(\gamma a^{\top}b+r)$ 
    

## Regresi√≥n SVM (SVR)

Las SVM tambi√©n pueden usarse para regresi√≥n.

- **Objetivo**: En lugar de separar clases, la regresi√≥n SVM intenta ajustar el mayor n√∫mero posible de instancias _dentro_ de la "calle", limitando las violaciones de margen.
    
- **Hiperpar√°metro $\epsilon$ (epsilon)**: La anchura de la "calle" se controla mediante el hiperpar√°metro `epsilon`.
    
- **$\epsilon$-insensible**: El modelo se considera $\epsilon$-insensible porque a√±adir m√°s instancias de entrenamiento _dentro_ del margen (la calle) no afecta a las predicciones del modelo.
    
- Para tareas de regresi√≥n no lineal, tambi√©n se pueden utilizar SVM kernelizadas.
    

## üìå Ventajas y Desventajas

‚úÖ **Ventajas**:

- El entrenamiento es relativamente f√°cil.
    
- No tiene √≥ptimos locales (a diferencia de las redes neuronales).
    
- Escala relativamente bien a datos de alta dimensionalidad.
    
- Permite usar kernels para datos no tradicionales (cadenas, √°rboles).
    

‚ùå **Desventajas**:

- Requiere la elecci√≥n de una funci√≥n "kernel" adecuada.
    

---

tags:

#Concept #MachineLearning #SVM

# Clasificaci√≥n SVM Lineal
Es un plano que separa los datos del superespacio.

El par√°metro C si o si hay que modificar lo al usar sckit-learn, siempre hay que ajustarlo. Sirve para ser mas permisivo con los puntos que crucen o est√©n dentro de la 'calle'. Cuanto mayor sea el valor de C, mas estrictos es.

Son procesos bastantes r√°pidos.
Se pueden a√±adir mas caracter√≠sticas para intentar mejorarlo.

Pero no suele funcionar.


Maximiar el margen equivale en minimizar el w.

# Clasificaci√≥n SVM no Lineal
## Kernel Polin√≥mico


Con SVC(kernel="linear") podemos elegir entre los 3 Kernels.


