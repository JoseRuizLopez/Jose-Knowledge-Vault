links: [[007 - 0310 Teoria|Teor칤a]]


# 츼rboles de Decisi칩n

Los **츼rboles de Decisi칩n** son algoritmos de aprendizaje autom치tico vers치tiles capaces de realizar tareas tanto de clasificaci칩n como de regresi칩n. Son algoritmos potentes, capaces de ajustar conjuntos de datos complejos, y son los componentes fundamentales de los _Random Forests_.

Los 치rboles de decisi칩n son intuitivos y sus decisiones son f치ciles de interpretar, por lo que se consideran modelos de **caja blanca** (_white box_).

## 游늷 C칩mo Funcionan

### Estructura y Predicciones

Un 치rbol se compone de varios tipos de nodos:

1. **Nodo Ra칤z (Root node)**: El nodo superior por el que se empieza.
    
2. **Nodo Partido (Split node)**: Un nodo que hace una pregunta (p.ej., "쯟ongitud del p칠talo <= 2.45cm?") y se divide en nodos hijos.
    
3. **Nodo Hoja (Leaf node)**: Un nodo terminal que no tiene hijos y que contiene la predicci칩n final (la clase o el valor).
    

Para hacer una predicci칩n, se recorre el 치rbol desde la ra칤z, respondiendo a las preguntas de cada nodo partido, hasta llegar a un nodo hoja.

## Algoritmo de Entrenamiento (CART)

Scikit-Learn utiliza el algoritmo **CART (Classification and Regression Tree)** para entrenar 치rboles de decisi칩n.

- El algoritmo CART solo produce **치rboles binarios** (nodos con exactamente dos hijos).
    
- Funciona dividiendo el conjunto de entrenamiento en dos subconjuntos usando una 칰nica caracter칤stica (k) y un umbral ($t_k$).
    
- Es un **algoritmo _greedy_** (codicioso): busca el par ($k$, $t_k$) que produce los subconjuntos m치s puros (ponderados por su tama침o) en el paso actual. Al ser _greedy_, suele producir una buena soluci칩n, pero no garantiza que sea la 칩ptima global.
    

### Medidas de Impureza (Clasificaci칩n)

El algoritmo mide la "pureza" de un nodo. Un nodo es "puro" ($gini=0$) si todas las instancias de entrenamiento a las que se aplica pertenecen a la misma clase.

- **Impureza de Gini** (default en Scikit-Learn):
- $$G_{i}=1-\sum_{k=1}^{n}{p_{i,k}}^{2}$$

    - Donde $p_{i,k}$ es la proporci칩n de instancias de clase _k_ en el nodo _i_.
        
    - Es ligeramente m치s r치pida de calcular que la entrop칤a.
        
    - Tiende a aislar la clase m치s frecuente en su propia rama.
        
- **Entrop칤a**:
    
    - Mide el desorden o la impureza.
  $$H_{i}=-\sum_{p_{i,k}=1}^{n}p_{i,k}\log_{2}(p_{i,k})$$
	- Tiende a producir 치rboles ligeramente m치s equilibrados.
        

## 츼rboles de Regresi칩n

Los 치rboles de decisi칩n tambi칠n son capaces de realizar tareas de regresi칩n.

- En lugar de predecir una clase en cada nodo hoja, predice un valor.
    
- El valor predicho es el **valor objetivo medio** de las instancias de entrenamiento asociadas a ese nodo hoja.
    
- El algoritmo CART funciona de manera similar, pero en lugar de minimizar la impureza (Gini o entrop칤a), divide el conjunto de entrenamiento para minimizar el **Error Cuadr치tico Medio (MSE)**.
    

## Regularizaci칩n y [[Overfitting]]

Los 치rboles de decisi칩n son **modelos no param칠tricos**; la estructura del modelo no se determina antes del entrenamiento, sino que se adapta libremente a los datos.

- Si no se imponen restricciones, el 치rbol se ajustar치 mucho a los datos de entrenamiento, lo que muy probablemente resulte en **[[Overfitting|sobreajuste]]**.
    
- Para evitar el sobreajuste, es necesario restringir la libertad del 치rbol durante el entrenamiento. Esto se logra mediante **hiperpar치metros de regularizaci칩n**:
    
    - `max_depth`: Restringe la profundidad m치xima del 치rbol (el valor por defecto `None` es ilimitado).
        
    - `min_samples_split`: N칰mero m칤nimo de muestras que debe tener un nodo para poder dividirse.
        
    - `min_samples_leaf`: N칰mero m칤nimo de muestras que debe tener un nodo hoja.
        
    - `max_features`: N칰mero m치ximo de caracter칤sticas a evaluar en cada divisi칩n.
        
    - `max_leaf_nodes`: N칰mero m치ximo de nodos hoja.
        

## 游늷 Limitaciones

Aunque potentes, los 치rboles de decisi칩n tienen algunas limitaciones:

- **Sensibilidad a la orientaci칩n del eje**: A los 치rboles de decisi칩n les gustan los l칤mites de decisi칩n ortogonales (paralelos a los ejes). Si los datos est치n rotados, el l칤mite de decisi칩n puede volverse innecesariamente complejo y no generalizar bien.
    
- **Alta Varianza**: Peque침os cambios en los datos o en los hiperpar치metros pueden producir modelos muy diferentes. El algoritmo de entrenamiento tambi칠n puede ser estoc치stico (aleatorio), lo que significa que reentrenar con los mismos datos puede dar un modelo diferente.
    

---

tags:

#Concept #MachineLearning #츼rbolesDeDecisi칩n