links: [[001 - 020 Machine Learning|Machine Learning]]


# K-Nearest Neighbors (KNN)

**K-Nearest Neighbors (KNN)**, o K-Vecinos m√°s Cercanos, es un algoritmo fundamental de [[001 - 020 Machine Learning|Machine Learning]] supervisado. Es un m√©todo **no param√©trico** (no asume una distribuci√≥n espec√≠fica de los datos) y de [[lazy learning]].

Su funcionamiento se basa en la premisa de "similitud de caracter√≠sticas": asume que los puntos de datos que son similares (vecinos) existen en proximidad. Por lo tanto, asigna una etiqueta a un nuevo punto de datos bas√°ndose en las etiquetas de sus vecinos m√°s cercanos.

## üìå C√≥mo funciona

A diferencia de muchos otros algoritmos, KNN **no tiene una fase de "entrenamiento"** expl√≠cita donde aprende una funci√≥n. Simplemente almacena todo el conjunto de datos. El c√°lculo real ocurre en el momento de la predicci√≥n (o "b√∫squeda").

El proceso de predicci√≥n sigue estos pasos:

1. **Almacenamiento de Datos**: El modelo guarda todas las instancias del conjunto de entrenamiento.
    
2. **C√°lculo de Distancia**: Cuando se introduce un nuevo punto (query) para predecir, el algoritmo calcula la distancia entre este punto y _todos_ los puntos de entrenamiento. La elecci√≥n de la m√©trica de distancia es crucial (ver secci√≥n siguiente).
    
3. **Identificaci√≥n de Vecinos**: Ordena todas las distancias de menor a mayor e identifica los **'K'** puntos de entrenamiento m√°s cercanos (los "K vecinos").
    
4. **Predicci√≥n (Votaci√≥n o Promedio)**:
    
    - **Para Clasificaci√≥n**: El algoritmo realiza una "votaci√≥n mayoritaria". El nuevo punto se asigna a la clase que m√°s se repite entre sus K vecinos.
        
    - **Para Regresi√≥n**: El algoritmo calcula el promedio (o a veces la mediana) de los valores de sus K vecinos y asigna ese resultado al nuevo punto.
        

---

## üîë Componentes Clave

### El valor 'K'

Es el n√∫mero de vecinos que se considerar√°n para la predicci√≥n. Es un [[Hiperpar√°metros|Hiperpar√°metro]] cr√≠tico que debe elegirse con cuidado:

- **K peque√±o** (ej. K=1): El modelo es muy flexible y se ajusta mucho al ruido de los datos, lo que puede llevar al [[Overfitting]].
    
- **K grande** (ej. K=N): El modelo se vuelve muy simple (siempre predice la clase/valor mayoritario de todo el dataset), lo que puede llevar al [[Underfitting]].
    

El valor de K suele elegirse usando t√©cnicas como [[Cross Validation]].

### M√©trica de Distancia

Define qu√© tan "cercanos" son dos puntos. La elecci√≥n depende de la naturaleza de los datos. Las m√°s comunes est√°n definidas en [[M√©tricas de Similitud]], como:

- **Distancia Euclidiana**: La distancia "recta" entre dos puntos. Es la m√°s com√∫n para datos num√©ricos de baja dimensi√≥n.
    
    $$ d(p, q) = \sqrt{\sum_{i=1}^n (p_i - q_i)^2}$$
    
- **Distancia de Manhattan**: La suma de las diferencias absolutas de las coordenadas. Es m√°s robusta ante _outliers_ que la Euclidiana.
    
    $$ d(p, q) = \sum_{i=1}^n |p_i - q_i|$$
    
- **Similitud de Coseno**: Mide el √°ngulo entre dos vectores. Es muy utilizada en datos de alta dimensi√≥n, como en el procesamiento de texto (NLP).
    

---

## ‚úÖ Ventajas y ‚ùå Desventajas

|**Ventajas**|**Desventajas**|
|---|---|
|‚úÖ **Simple e intuitivo**: F√°cil de entender y explicar.|‚ùå **Coste computacional**: Lento en la predicci√≥n, ya que debe comparar el nuevo punto con _todos_ los puntos de entrenamiento.|
|‚úÖ **Sin fase de entrenamiento**: Es un algoritmo _lazy_, los nuevos datos se pueden agregar f√°cilmente.|‚ùå **"Maldici√≥n de la Dimensionalidad"**: Pierde efectividad a medida que aumenta el n√∫mero de caracter√≠sticas (dimensiones).|
|‚úÖ **Vers√°til**: Funciona tanto para clasificaci√≥n como para regresi√≥n.|‚ùå **Sensible al escalado**: Requiere [[normalizaci√≥n]] o estandarizaci√≥n de caracter√≠sticas, ya que las variables con rangos m√°s grandes dominar√°n las distancias.|
|‚úÖ **No param√©trico**: No hace suposiciones sobre la distribuci√≥n de los datos.|‚ùå **Sensible a datos desbalanceados**: En clasificaci√≥n, la clase mayoritaria tiende a dominar la votaci√≥n. Requiere t√©cnicas de [[Balanceo de Clases]].|

---

## üí° Aplicaciones

- **Sistemas de Recomendaci√≥n**: Sugerir productos bas√°ndose en lo que compraron "usuarios similares" (vecinos).
    
- **B√∫squeda Sem√°ntica**: Encontrar los _embeddings_ (vectores) m√°s cercanos a una consulta de texto o imagen.
    
- **Clasificaci√≥n de Documentos**: Asignar un t√≥pico a un documento bas√°ndose en los t√≥picos de documentos similares.
    
- **Detecci√≥n de Anomal√≠as**: Identificar puntos cuyos vecinos m√°s cercanos est√°n muy lejos, sugiriendo que son _outliers_.
    

---

tags:

#Concept #MachineLearning #Algoritmo #Clasificacion #Regresion