links: [[001 - 020 Machine Learning|Machine Learning]]

# Batch Normalization (BN)
**Batch Normalization (BN)** es una tÃ©cnica utilizada en redes neuronales profundas para acelerar el entrenamiento y mejorar la estabilidad del modelo. 

## ðŸ“Œ Â¿CÃ³mo funciona?

Batch Normalization normaliza las activaciones de una capa antes de pasarlas a la siguiente, reduciendo la **varianza interna del covariado** (Internal Covariate Shift). Esto se hace en cuatro pasos:

1. **CÃ¡lculo de la media y varianza** de las activaciones dentro de un mini-lote (_batch_).
    $\mu_B = \frac{1}{m} \sum_{i=1}^{m} x_i$
    
    $Ïƒ^2_B = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2$
2. **NormalizaciÃ³n**: Se reescalan los valores para que tengan media 0 y varianza 1.
	$\hat{x}_I = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}â€‹â€‹$
    
    $\epsilon$ es un valor pequeÃ±o para evitar divisiones por cero.
    
3. **Reescalado y desplazamiento** con parÃ¡metros aprendibles (Î³\gammaÎ³ y Î²\betaÎ²). Esto permite que la red pueda ajustar la distribuciÃ³n si es necesario.
    $y_i = \gamma \hat{x}_i + \beta$
4. **Se usa la salida normalizada en la siguiente capa de la red**.


## ðŸ“Œ Â¿Por quÃ© es Ãºtil?

âœ… **Acelera el entrenamiento**: Reduce la dependencia del ajuste de la tasa de aprendizaje.  
âœ… **Evita problemas de saturaciÃ³n** en funciones de activaciÃ³n como Sigmoide o Tanh.  
âœ… **Reduce la sensibilidad a la inicializaciÃ³n de los pesos**.  
âœ… **ActÃºa como una regularizaciÃ³n** adicional, reduciendo la necesidad de _Dropout_.

## ðŸ“Œ Â¿DÃ³nde se usa Batch Normalization?

- En capas ocultas de redes neuronales profundas.
- En modelos como CNNs y Transformers.
- En entrenamiento con mini-batches.




---
tags:
	#Concept  #MachineLearning