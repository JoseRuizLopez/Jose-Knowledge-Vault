links: [[001 - 020 Machine Learning|Machine Learning]]

# Overfitting
El sobreajuste (overfitting)Â es el efecto de sobreentrenar un algoritmo de aprendizaje con unos ciertos datos para los que se conoce el resultado deseado, de esta forma el modelo aprende demasiado bien los detalles y ruido del conjunto de entrenamiento, perdiendo capacidad de generalizaciÃ³n en datos nuevos.

## ğŸ” CÃ³mo prevenirlo
MÃ©todos adicionales para manejar overfitting incluyen dropout, regularizaciÃ³n L1/L2 y reducciÃ³n de la complejidad del modelo.

- **Cantidad mÃ­nima de muestras**Â tanto para entrenar el modelo como para validarlo.
- **Clases variadas y equilibradas en cantidad**: En caso deÂ aprendizaje supervisadoÂ y suponiendo que tenemos queÂ clasificarÂ diversasÂ clases o categorÃ­as,Â es importante queÂ los datos de entrenamiento estÃ©n balanceados. Supongamos que tenemos que diferenciar entre manzanas, peras y bananas, debemos tener muchas fotos de las 3 frutas y en cantidades similares. Â Si tenemos muy pocas fotos de peras, esto afectarÃ¡ en el aprendizaje de nuestro algoritmo para identificar esa fruta.
- **Conjunto de Test de datos**. Siempre subdividir nuestro conjunto de datos y mantener una porciÃ³n del mismo â€œocultoâ€ a nuestra mÃ¡quina entrenada.Â Esto nos permitirÃ¡ obtener una valoraciÃ³n de aciertos/fallos real del modelo y tambiÃ©n nos permitirÃ¡ detectar fÃ¡cilmente efectos del [[Overfitting]] /[[Underfitting]].
- **Parameter TunningÂ o Ajuste de ParÃ¡metros**: deberemos experimentar sobre todo dando mÃ¡s/menos â€œtiempo/iteracionesâ€ al entrenamiento y su aprendizaje hasta encontrar el equilibrio.
- **Cantidad excesiva de Dimensiones**Â (features), con muchas variantes distintas, sin suficientes muestras. A veces conviene eliminar o reducir la cantidad de caracterÃ­sticas que utilizaremos para entrenar el modelo. UnaÂ herramienta Ãºtil para hacerlo es *PCA*
- Quiero notar que si nuestro modelo es unaÂ red neuronal artificialâ€“deep learning-, podemos caer en overfittingÂ **si usamos capas ocultas en exceso**, ya queÂ harÃ­amos que el modelo memorice las posibles salidas, en vez de ser flexible y adecuar las activaciones a las entradas nuevas.

---
tags:
	#Concept  #MachineLearning 