links: [[008 Summaries|Summaries]] 


Reference: [[HAKE.pdf|HAKE: A Knowledge Engine Foundation for Human Activity Understanding]]
# Resumen del HAKE
**Problema:**  
Los modelos de visi√≥n profunda tradicionales (CNN, DETR, etc.) funcionan bien para reconocer **objetos**, pero fallan al interpretar **actividades humanas** (como _‚Äúlavar el coche‚Äù_ o _‚Äúmontar a caballo‚Äù_).  
Las actividades requieren **relaciones entre partes del cuerpo, objetos y acciones**; no basta con detectar p√≠xeles o categor√≠as.

**Hip√≥tesis clave:**  
El reconocimiento de actividades humanas necesita un nivel intermedio de representaci√≥n ‚Äîlos llamados **primitivos**‚Äî que describen **estados sem√°nticos de partes del cuerpo y objetos**.  
Luego, un **motor simb√≥lico** combina estos primitivos con **reglas l√≥gicas** (AND, OR, NOT) para inferir la actividad final.

> üëâ En resumen:  
> Imagen ‚Üí Detecci√≥n de primitivos ‚Üí Razonamiento simb√≥lico ‚Üí Actividad reconocida + explicaci√≥n.

## Arquitectura general

HAKE propone una **arquitectura neuro-simb√≥lica en dos etapas**:

### (1) Detecci√≥n de primitivos

- Usa CNNs (como Faster R-CNN o Swin Transformer) para detectar **partes del cuerpo** (cabeza, brazos, manos, piernas‚Ä¶) y **objetos**.
    
- Cada parte obtiene una serie de **estados sem√°nticos (PaSta, Part States)** como:
    
    - `hand-hold-sth`, `head-eat-sth`, `leg-bend`, `foot-stand-on-sth`, etc.
        
- Estos ‚Äúprimitivos‚Äù representan **acciones at√≥micas transferibles** entre actividades.
    

HAKE construye una **base de conocimiento gigante** con:

- 357 000 im√°genes / frames.
    
- 26 millones de etiquetas primitivas.
    
- 156 clases de actividades (combinaciones de verbos y objetos).
    

Cada primitivo se etiqueta manualmente y con crowdsourcing.


### (2) Razonamiento simb√≥lico

- Los primitivos detectados se **programan mediante reglas l√≥gicas**.
    
- Ejemplo de regla:
    
    `(hand-hold-sth) ‚àß (head-eat-sth) ‚àß (object=apple) ‚Üí activity=eat apple`
    
- Usa dos m√≥dulos MLP diferenciables para implementar las operaciones:
    
    - **NOT(x)** ‚Üí negaci√≥n.
        
    - **OR(x, y)** ‚Üí disyunci√≥n.
        
- Estas operaciones se entrenan con **regularizaci√≥n l√≥gica** para cumplir leyes como:
    
    - ¬¨¬¨x = x
        
    - x ‚à® x = x
        
    - x ‚à® ¬¨x = True
        
- Se calculan **p√©rdidas por consistencia l√≥gica** + **p√©rdidas de clasificaci√≥n**.
    

> Resultado: un **modelo diferenciable** que razona simb√≥licamente pero entrena con backpropagation.


## M√≥dulos clave

### üîπ a) Activity2Vec

Transforma cada primitivo a un vector sem√°ntico combinando:

- Representaci√≥n visual (de CNN).
    
- Representaci√≥n ling√º√≠stica (usando BERT).
    

Permite razonar con una representaci√≥n multimodal que mezcla visi√≥n + lenguaje.


### üîπ b) Motor de reglas (Inductivo‚ÄìDeductivo)

- Empieza con reglas simb√≥licas b√°sicas escritas por humanos (‚Äúhuman prior‚Äù).
    
- Luego **descubre nuevas reglas autom√°ticamente**:
    
    - **Inducci√≥n:** genera reglas candidatas observando co-ocurrencias entre primitivos y actividades.
        
    - **Deducci√≥n:** eval√∫a esas reglas seg√∫n la p√©rdida durante el entrenamiento y conserva las m√°s efectivas.
        
- Este proceso iterativo produce una **base de reglas din√°mica y autoajustable** (‚âà 4 000 reglas finales).
    


### üîπ c) Integraci√≥n con modelos neuronales existentes

HAKE no reemplaza a las CNN/Transformers, sino que se **acopla encima**:

- Puede mejorar modelos previos como TIN, QPIC o SlowFast.
    
- Fusiona resultados neuronales (`S_ins`) y simb√≥licos (`S_LR`) como:  
    `S_total = S_ins √ó S_LR`
    



## Resultados experimentales

### Datasets

- **HICO-DET** ‚Üí im√°genes de interacciones humano-objeto.
    
- **V-COCO** ‚Üí detecci√≥n de acciones sobre objetos (basado en COCO).
    
- **AVA** ‚Üí actividades en v√≠deo.
    
- **Ambiguous-HOI** ‚Üí escenas ambiguas para poner a prueba el razonamiento.
    



### Rendimiento

|Dataset|Baseline|+ HAKE|Mejora relativa|
|---|---|---|---|
|HICO-DET|TIN: 17.0 mAP|23.2 mAP|+36 %|
|HICO-DET|VCL: 23.6 mAP|28.3 mAP|+20 %|
|AVA|SlowFast: 28.2 mAP|29.3 mAP|+4 %|
|V-COCO|TIN: 54.2 mAP|59.7 mAP|+10 %|

Con **primitivos perfectos (GT)** llega a **~72 mAP**, lo que marca el _upper bound_ del sistema.

Adem√°s, demuestra **gran capacidad de transferencia** entre datasets sin reentrenar, gracias a su representaci√≥n simb√≥lica general.



## Insights te√≥ricos y pr√°cticos

1. **Los primitivos son la clave de la composicionalidad.**  
    ‚Üí Permiten que el modelo generalice a combinaciones nuevas (_wash horse_ tras ver _wash car_ y _ride horse_).
    
2. **El razonamiento l√≥gico aumenta la explicabilidad.**  
    ‚Üí Cada decisi√≥n puede trazarse mediante reglas (no una caja negra).
    
3. **El aprendizaje inductivo-deductivo descubre conocimiento.**  
    ‚Üí El modelo puede aprender reglas nuevas (‚Äúsi hay ski\_board y leg-bend, probablemente es _ride ski_board_‚Äù).
    
4. **Es un sistema diferenciable y escalable.**  
    ‚Üí Integra l√≥gica y deep learning sin sacrificar el entrenamiento end-to-end.
    



## Limitaciones y futuro

- Dependencia de anotaciones costosas (26 M primitivos).
    
- El m√≥dulo l√≥gico a√∫n es simplificado (solo usa ¬¨, ‚à®).
    
- Puede fallar en razonamientos multi-paso o en escenas sin partes corporales claras.
    
- Futuro: integrar **Knowledge Graphs** externos (commonsense), **ontolog√≠as [[OWL]]** y **razonamiento simb√≥lico m√°s completo (FOL o PSL)**.
    



## Conexi√≥n directa con el [[102 - no-TFM]]

|Objetivo de tu TFM|Relaci√≥n con HAKE|
|---|---|
|**Reconocimiento sem√°ntico de escenas**|HAKE interpreta interacciones complejas (humano-objeto) mediante representaci√≥n simb√≥lica.|
|**Integraci√≥n DL + KG**|Su motor de reglas es un primer paso hacia razonamiento sobre un grafo de conocimiento.|
|**Explicabilidad simb√≥lica**|Explica cada predicci√≥n con l√≥gica humana (`A ‚àß B ‚Üí C`).|
|**Evaluaci√≥n de generalizaci√≥n sistem√°tica**|Puede probarse con los _splits_ HICO-DET-SG y V-COCO-SG.|
|**Modelo neuro-simb√≥lico**|Combina CNNs/Transformers (percepci√≥n) con razonamiento l√≥gico (s√≠mbolo).|



## En resumen

> **HAKE transforma la comprensi√≥n de actividades humanas en una tarea neuro-simb√≥lica jer√°rquica:**  
> detecta **primitivos visuales**, los **combina mediante reglas l√≥gicas**, y obtiene **actividades explicables y generalizables**.

**Contribuye a el [[102 - no-TFM]]** como ejemplo de:

- Integraci√≥n expl√≠cita de conocimiento simb√≥lico en visi√≥n.
    
- Arquitectura diferenciable DL + l√≥gica.
    
- Modelo ideal para extender con **Knowledge Graphs espaciales/funcionales** en _scene understanding_.


---
tags:
	#Writing #Summary